{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6116ea82",
   "metadata": {},
   "source": [
    "# Stock Market Prediction Model Prototyping\n",
    "\n",
    "This notebook focuses on prototyping different machine learning models for stock price prediction. We'll implement and compare various approaches, from simple baseline models to more sophisticated deep learning architectures.\n",
    "\n",
    "**Contents:**\n",
    "1. Data Loading and Preparation\n",
    "2. Baseline Models\n",
    "3. Deep Learning Models\n",
    "4. Model Comparison\n",
    "5. Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e66329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, MultiHeadAttention\n",
    "from tensorflow.keras.layers import LayerNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Add the src directory to the path so we can import our modules\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1da2ab",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation\n",
    "\n",
    "We'll load processed stock data that has already been prepared with feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "processed_data_path = '../data/processed/'\n",
    "\n",
    "# List available processed data files\n",
    "npz_files = [f for f in os.listdir(processed_data_path) if f.endswith('_ml_ready.npz')]\n",
    "print(f\"Found {len(npz_files)} processed data files.\")\n",
    "\n",
    "# If no processed files are found, we might need to run data preparation\n",
    "if len(npz_files) == 0:\n",
    "    print(\"No processed data files found. You may need to run the data preprocessing script first.\")\n",
    "    print(\"Run: python -m src.preprocess\")\n",
    "else:\n",
    "    # Select a stock symbol for prototyping (e.g., AAPL if available)\n",
    "    preferred_symbols = ['AAPL', 'MSFT', 'GOOG', 'AMZN']\n",
    "    available_symbols = [os.path.splitext(f)[0].replace('_ml_ready', '') for f in npz_files]\n",
    "    \n",
    "    selected_symbol = next((symbol for symbol in preferred_symbols if symbol in str(available_symbols)), available_symbols[0])\n",
    "    print(f\"Selected symbol for prototyping: {selected_symbol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for the selected symbol\n",
    "def load_data(symbol):\n",
    "    \"\"\"Load processed data for a specific stock symbol.\"\"\"\n",
    "    data_file = os.path.join(processed_data_path, f\"{symbol}_ml_ready.npz\")\n",
    "    \n",
    "    if not os.path.exists(data_file):\n",
    "        raise FileNotFoundError(f\"Processed data file not found: {data_file}\")\n",
    "    \n",
    "    data = np.load(data_file)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    dates = data['dates']\n",
    "    \n",
    "    return X, y, dates\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    X, y, dates = load_data(selected_symbol)\n",
    "    print(f\"Data loaded successfully. X shape: {X.shape}, y shape: {y.shape}, dates shape: {dates.shape}\")\n",
    "    \n",
    "    # Convert dates to datetime if needed\n",
    "    if isinstance(dates[0], bytes):\n",
    "        dates = [d.decode('utf-8') for d in dates]  # Convert bytes to strings if needed\n",
    "        dates = [pd.to_datetime(d) for d in dates]  # Convert strings to datetime\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "def split_data(X, y, dates, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"Split data into training, validation, and test sets using time-based split.\"\"\"\n",
    "    # Time-based split (no shuffling)\n",
    "    n_samples = len(X)\n",
    "    test_idx = int(n_samples * (1 - test_size))\n",
    "    \n",
    "    X_temp, X_test = X[:test_idx], X[test_idx:]\n",
    "    y_temp, y_test = y[:test_idx], y[test_idx:]\n",
    "    dates_temp, dates_test = dates[:test_idx], dates[test_idx:]\n",
    "    \n",
    "    val_idx = int(len(X_temp) * (1 - val_size))\n",
    "    \n",
    "    X_train, X_val = X_temp[:val_idx], X_temp[val_idx:]\n",
    "    y_train, y_val = y_temp[:val_idx], y_temp[val_idx:]\n",
    "    dates_train, dates_val = dates_temp[:val_idx], dates_temp[val_idx:]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, dates_train, dates_val, dates_test\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, dates_train, dates_val, dates_test = split_data(X, y, dates)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc6ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data split\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Concatenate all target values and dates\n",
    "all_dates = np.concatenate([dates_train, dates_val, dates_test])\n",
    "all_prices = np.concatenate([y_train, y_val, y_test])\n",
    "\n",
    "# Plot the full price series\n",
    "plt.plot(all_dates, all_prices, color='gray', alpha=0.3, label='All Data')\n",
    "\n",
    "# Plot the training, validation, and test sets\n",
    "plt.plot(dates_train, y_train, color='blue', label='Training Set')\n",
    "plt.plot(dates_val, y_val, color='green', label='Validation Set')\n",
    "plt.plot(dates_test, y_test, color='red', label='Test Set')\n",
    "\n",
    "# Add vertical lines to separate the sets\n",
    "plt.axvline(x=dates_train[-1], color='black', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=dates_val[-1], color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.title(f'{selected_symbol} - Data Split', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Price', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93937391",
   "metadata": {},
   "source": [
    "## 2. Baseline Models\n",
    "\n",
    "We'll implement and evaluate simple baseline models for comparison:\n",
    "- Last value prediction (naive approach)\n",
    "- Linear Regression\n",
    "- ARIMA time series model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate model performance\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate model predictions using various metrics.\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RÂ²: {r2:.4f}\")\n",
    "    print(f\"MAPE: {mape:.4f}%\")\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape,\n",
    "        'y_pred': y_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbe428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Naive Model (predict the last observed value)\n",
    "def naive_forecast(X_test):\n",
    "    \"\"\"Use the last observed price as the prediction for the next day.\"\"\"\n",
    "    # Get the last closing price from each window\n",
    "    close_price_column = -1  # Assuming the closing price is the last feature\n",
    "    last_observed_prices = X_test[:, -1, close_price_column]\n",
    "    return last_observed_prices\n",
    "\n",
    "# Make naive predictions\n",
    "naive_preds = naive_forecast(X_test)\n",
    "\n",
    "# Evaluate the naive model\n",
    "naive_results = evaluate_model(y_test, naive_preds, \"Naive Last Value Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Linear Regression Model\n",
    "def fit_linear_model(X_train, y_train, X_test):\n",
    "    \"\"\"Fit a linear regression model and make predictions.\"\"\"\n",
    "    # Reshape 3D data to 2D for sklearn\n",
    "    n_samples_train, n_timesteps, n_features = X_train.shape\n",
    "    X_train_2d = X_train.reshape(n_samples_train, n_timesteps * n_features)\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X_train_scaled = scaler_X.fit_transform(X_train_2d)\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Fit the linear model\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_scaled, y_train_scaled)\n",
    "    \n",
    "    # Prepare test data\n",
    "    n_samples_test, _, _ = X_test.shape\n",
    "    X_test_2d = X_test.reshape(n_samples_test, n_timesteps * n_features)\n",
    "    X_test_scaled = scaler_X.transform(X_test_2d)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_scaled = lr_model.predict(X_test_scaled)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return y_pred, lr_model\n",
    "\n",
    "# Use the linear regression model\n",
    "start_time = time.time()\n",
    "lr_preds, lr_model = fit_linear_model(X_train, y_train, X_test)\n",
    "lr_time = time.time() - start_time\n",
    "\n",
    "print(f\"Linear Regression training time: {lr_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate the linear regression model\n",
    "lr_results = evaluate_model(y_test, lr_preds, \"Linear Regression Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edee7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ARIMA Model\n",
    "def fit_arima_model(y_train, test_size):\n",
    "    \"\"\"Fit an ARIMA model to the training data and make predictions.\"\"\"\n",
    "    # Convert to pandas Series for statsmodels\n",
    "    train_series = pd.Series(y_train)\n",
    "    \n",
    "    # Fit ARIMA model\n",
    "    order = (5, 1, 0)  # AR(5), I(1), MA(0) - common for stock prices\n",
    "    model = sm.tsa.ARIMA(train_series, order=order)\n",
    "    \n",
    "    # This can be slow, so we'll show a message\n",
    "    print(\"Fitting ARIMA model... (this may take a few minutes)\")\n",
    "    fit_model = model.fit()\n",
    "    print(\"ARIMA model fit complete.\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = fit_model.forecast(steps=test_size)\n",
    "    \n",
    "    return y_pred, fit_model\n",
    "\n",
    "# Use the ARIMA model\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    arima_preds, arima_model = fit_arima_model(y_train, len(y_test))\n",
    "    arima_time = time.time() - start_time\n",
    "\n",
    "    print(f\"ARIMA training and prediction time: {arima_time:.2f} seconds\")\n",
    "\n",
    "    # Evaluate the ARIMA model\n",
    "    arima_results = evaluate_model(y_test, arima_preds, \"ARIMA Model\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fitting ARIMA model: {e}\")\n",
    "    arima_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ff360",
   "metadata": {},
   "source": [
    "## 3. Deep Learning Models\n",
    "\n",
    "Now we'll implement and evaluate more sophisticated models:\n",
    "- LSTM (Long Short-Term Memory)\n",
    "- Transformer (with self-attention mechanism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common preprocessing for deep learning models\n",
    "def preprocess_for_dl(X_train, y_train, X_val, y_val, X_test):\n",
    "    \"\"\"Preprocess data for deep learning models (scaling).\"\"\"\n",
    "    # Reshape X data for scaling\n",
    "    n_samples_train, n_timesteps, n_features = X_train.shape\n",
    "    X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "    X_val_reshaped = X_val.reshape(-1, n_features)\n",
    "    X_test_reshaped = X_test.reshape(-1, n_features)\n",
    "    \n",
    "    # Scale features\n",
    "    X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train_scaled = X_scaler.fit_transform(X_train_reshaped)\n",
    "    X_val_scaled = X_scaler.transform(X_val_reshaped)\n",
    "    X_test_scaled = X_scaler.transform(X_test_reshaped)\n",
    "    \n",
    "    # Reshape back to 3D\n",
    "    X_train_scaled = X_train_scaled.reshape(n_samples_train, n_timesteps, n_features)\n",
    "    X_val_scaled = X_val_scaled.reshape(X_val.shape[0], n_timesteps, n_features)\n",
    "    X_test_scaled = X_test_scaled.reshape(X_test.shape[0], n_timesteps, n_features)\n",
    "    \n",
    "    # Scale targets\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, X_test_scaled, y_scaler\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, X_test_scaled, y_scaler = preprocess_for_dl(\n",
    "    X_train, y_train, X_val, y_val, X_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b983f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LSTM Model\n",
    "def build_lstm_model(input_shape, units=64, layers=2, dropout_rate=0.2):\n",
    "    \"\"\"Build an LSTM model with the specified architecture.\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer\n",
    "    model.add(LSTM(units=units, \n",
    "                   return_sequences=(layers > 1),\n",
    "                   input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Additional LSTM layers\n",
    "    for i in range(layers - 1):\n",
    "        return_seq = (i < layers - 2)  # Return sequences for all but the last layer\n",
    "        model.add(LSTM(units=units, return_sequences=return_seq))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint_path = \"../models/lstm/lstm_checkpoint.h5\"\n",
    "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Build and train the LSTM model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "lstm_model = build_lstm_model(input_shape, units=64, layers=2)\n",
    "\n",
    "# Show model summary\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    validation_data=(X_val_scaled, y_val_scaled),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "lstm_time = time.time() - start_time\n",
    "print(f\"LSTM training time: {lstm_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "lstm_preds_scaled = lstm_model.predict(X_test_scaled)\n",
    "lstm_preds = y_scaler.inverse_transform(lstm_preds_scaled).flatten()\n",
    "\n",
    "# Evaluate the LSTM model\n",
    "lstm_results = evaluate_model(y_test, lstm_preds, \"LSTM Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Transformer Model\n",
    "def transformer_encoder_block(inputs, head_size, num_heads, ff_dim, dropout_rate=0.2):\n",
    "    \"\"\"Create a transformer encoder block.\"\"\"\n",
    "    # Multi-head self-attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=head_size\n",
    "    )(inputs, inputs)\n",
    "    attention_output = Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(attention_output)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "    return LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)\n",
    "\n",
    "def build_transformer_model(input_shape, head_size=256, num_heads=4, ff_dim=512, \n",
    "                            num_transformer_blocks=2, mlp_units=[128, 64], dropout_rate=0.2):\n",
    "    \"\"\"Build a transformer model for time series forecasting.\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    # Transformer blocks\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder_block(x, head_size, num_heads, ff_dim, dropout_rate)\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # MLP layers\n",
    "    for dim in mlp_units:\n",
    "        x = Dense(dim, activation=\"relu\")(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mae\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and train the Transformer model\n",
    "transformer_model = build_transformer_model(input_shape)\n",
    "\n",
    "# Show model summary\n",
    "transformer_model.summary()\n",
    "\n",
    "# Set up early stopping and model checkpoint\n",
    "checkpoint_path = \"../models/transformer/transformer_checkpoint.h5\"\n",
    "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "transformer_history = transformer_model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    validation_data=(X_val_scaled, y_val_scaled),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "transformer_time = time.time() - start_time\n",
    "print(f\"Transformer training time: {transformer_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "transformer_preds_scaled = transformer_model.predict(X_test_scaled)\n",
    "transformer_preds = y_scaler.inverse_transform(transformer_preds_scaled).flatten()\n",
    "\n",
    "# Evaluate the Transformer model\n",
    "transformer_results = evaluate_model(y_test, transformer_preds, \"Transformer Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50488ff7",
   "metadata": {},
   "source": [
    "## 4. Model Comparison\n",
    "\n",
    "Now we'll compare all the models to see which performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = [\n",
    "    naive_results,\n",
    "    lr_results,\n",
    "    lstm_results,\n",
    "    transformer_results\n",
    "]\n",
    "\n",
    "# Add ARIMA results if available\n",
    "if 'arima_results' in locals() and arima_results is not None:\n",
    "    all_results.append(arima_results)\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_metrics = pd.DataFrame([\n",
    "    {\n",
    "        'Model': r['model'],\n",
    "        'MSE': r['mse'],\n",
    "        'RMSE': r['rmse'],\n",
    "        'MAE': r['mae'],\n",
    "        'RÂ²': r['r2'],\n",
    "        'MAPE (%)': r['mape']\n",
    "    }\n",
    "    for r in all_results\n",
    "])\n",
    "\n",
    "# Display the summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "display(summary_metrics.sort_values('RMSE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3bcc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "def plot_model_comparison(results_list, metric_name):\n",
    "    \"\"\"Create a bar chart comparing models on a specific metric.\"\"\"\n",
    "    models = [r['model'] for r in results_list]\n",
    "    metric_values = [r[metric_name.lower()] for r in results_list]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(models, metric_values)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., \n",
    "                 value + 0.002*max(metric_values), \n",
    "                 f\"{value:.4f}\", \n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.title(f'Model Comparison - {metric_name}', fontsize=16)\n",
    "    plt.ylabel(metric_name, fontsize=14)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot comparisons for key metrics\n",
    "plot_model_comparison(all_results, \"RMSE\")\n",
    "plot_model_comparison(all_results, \"MAE\")\n",
    "plot_model_comparison(all_results, \"R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for the best model\n",
    "def plot_predictions(y_true, y_pred, dates, model_name):\n",
    "    \"\"\"Plot model predictions against actual values.\"\"\"\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    plt.plot(dates, y_true, label='Actual', linewidth=2)\n",
    "    plt.plot(dates, y_pred, label=f'{model_name} Prediction', linewidth=2, linestyle='--')\n",
    "    \n",
    "    # Add shaded region for prediction error\n",
    "    plt.fill_between(dates, y_true, y_pred, color='gray', alpha=0.3, label='Error')\n",
    "    \n",
    "    plt.title(f'{selected_symbol} Stock Price Prediction - {model_name}', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "    plt.ylabel('Price', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Find the best model based on RMSE\n",
    "best_model_idx = summary_metrics['RMSE'].idxmin()\n",
    "best_model = summary_metrics.loc[best_model_idx, 'Model']\n",
    "best_results = next(r for r in all_results if r['model'] == best_model)\n",
    "\n",
    "# Plot the best model's predictions\n",
    "plot_predictions(y_test, best_results['y_pred'], dates_test, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba49a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for all models\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot actual values\n",
    "plt.plot(dates_test, y_test, label='Actual', linewidth=2.5, color='black')\n",
    "\n",
    "# Plot predictions for each model with different colors\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "for i, result in enumerate(all_results):\n",
    "    plt.plot(dates_test, result['y_pred'], label=f\"{result['model']} Prediction\", \n",
    "             linestyle='--', linewidth=1.5, alpha=0.8, color=colors[i % len(colors)])\n",
    "\n",
    "plt.title(f'{selected_symbol} Stock Price Predictions - Model Comparison', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Price', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14089d09",
   "metadata": {},
   "source": [
    "## 5. Ablation Studies\n",
    "\n",
    "Let's analyze how different hyperparameters affect model performance, focusing on the best deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2bfe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss curves\n",
    "def plot_loss_curves(history, model_name):\n",
    "    \"\"\"Plot the training and validation loss curves.\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Loss Curves', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title(f'{model_name} - MAE Curves', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Mean Absolute Error', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for overfitting\n",
    "    min_val_loss_idx = np.argmin(history.history['val_loss'])\n",
    "    min_val_loss = history.history['val_loss'][min_val_loss_idx]\n",
    "    min_train_loss = history.history['loss'][min_val_loss_idx]\n",
    "    \n",
    "    print(f\"Best epoch: {min_val_loss_idx+1}\")\n",
    "    print(f\"Training loss: {min_train_loss:.4f}\")\n",
    "    print(f\"Validation loss: {min_val_loss:.4f}\")\n",
    "    print(f\"Difference: {(min_train_loss - min_val_loss):.4f}\")\n",
    "    \n",
    "    if min_train_loss < min_val_loss:\n",
    "        gap_percent = (min_val_loss - min_train_loss) / min_val_loss * 100\n",
    "        if gap_percent > 10:\n",
    "            print(f\"Warning: Possible overfitting detected. Train-Val gap: {gap_percent:.2f}%\")\n",
    "        else:\n",
    "            print(\"Model seems well-fit (no significant overfitting detected)\")\n",
    "    else:\n",
    "        print(\"Interesting: Validation loss is lower than training loss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves for the models\n",
    "if 'lstm_history' in locals():\n",
    "    plot_loss_curves(lstm_history, 'LSTM')\n",
    "\n",
    "if 'transformer_history' in locals():\n",
    "    plot_loss_curves(transformer_history, 'Transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcbfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model complexity\n",
    "def print_model_complexity(models_dict):\n",
    "    \"\"\"Analyze and print the complexity of each model.\"\"\"\n",
    "    complexity_data = []\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        if hasattr(model, 'count_params'):\n",
    "            params = model.count_params()\n",
    "            trainable_params = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "            non_trainable_params = params - trainable_params\n",
    "            \n",
    "            complexity_data.append({\n",
    "                'Model': name,\n",
    "                'Total Parameters': params,\n",
    "                'Trainable Parameters': trainable_params,\n",
    "                'Non-trainable Parameters': non_trainable_params\n",
    "            })\n",
    "    \n",
    "    if complexity_data:\n",
    "        complexity_df = pd.DataFrame(complexity_data)\n",
    "        display(complexity_df)\n",
    "    else:\n",
    "        print(\"No compatible models found for complexity analysis.\")\n",
    "\n",
    "# Analyze model complexity\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "models_dict = {\n",
    "    \"LSTM\": lstm_model,\n",
    "    \"Transformer\": transformer_model\n",
    "}\n",
    "\n",
    "print_model_complexity(models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a5bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation study on LSTM hyperparameters\n",
    "def run_lstm_ablation(X_train, y_train, X_val, y_val, X_test, y_test, y_scaler, param_grid):\n",
    "    \"\"\"Run an ablation study on LSTM hyperparameters.\"\"\"\n",
    "    results = []\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    for units in param_grid['units']:\n",
    "        for layers in param_grid['layers']:\n",
    "            for dropout in param_grid['dropout_rate']:\n",
    "                print(f\"\\nTesting LSTM with units={units}, layers={layers}, dropout={dropout}\")\n",
    "                \n",
    "                # Build and compile the model\n",
    "                model = build_lstm_model(\n",
    "                    input_shape=input_shape,\n",
    "                    units=units,\n",
    "                    layers=layers,\n",
    "                    dropout_rate=dropout\n",
    "                )\n",
    "                \n",
    "                # Set up callbacks\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "                \n",
    "                # Train the model\n",
    "                start_time = time.time()\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=30,  # Reduced for speed\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=0\n",
    "                )\n",
    "                training_time = time.time() - start_time\n",
    "                \n",
    "                # Make predictions\n",
    "                preds_scaled = model.predict(X_test, verbose=0)\n",
    "                preds = y_scaler.inverse_transform(preds_scaled).flatten()\n",
    "                \n",
    "                # Evaluate\n",
    "                mse = mean_squared_error(y_test, preds)\n",
    "                rmse = np.sqrt(mse)\n",
    "                mae = mean_absolute_error(y_test, preds)\n",
    "                \n",
    "                # Get parameter count\n",
    "                params = model.count_params()\n",
    "                \n",
    "                # Add to results\n",
    "                results.append({\n",
    "                    'Units': units,\n",
    "                    'Layers': layers,\n",
    "                    'Dropout': dropout,\n",
    "                    'Parameters': params,\n",
    "                    'Training Time': training_time,\n",
    "                    'MSE': mse,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAE': mae,\n",
    "                    'Final Train Loss': history.history['loss'][-1],\n",
    "                    'Final Val Loss': history.history['val_loss'][-1],\n",
    "                    'Epochs': len(history.history['loss'])\n",
    "                })\n",
    "                \n",
    "                print(f\"RMSE: {rmse:.4f}, Training Time: {training_time:.2f}s\")\n",
    "                \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define the parameter grid for the ablation study\n",
    "lstm_param_grid = {\n",
    "    'units': [32, 64, 128],\n",
    "    'layers': [1, 2],\n",
    "    'dropout_rate': [0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Run the ablation study (commented out by default to save time)\n",
    "# You can uncomment and run this cell if you want to do the ablation study\n",
    "'''\n",
    "ablation_results = run_lstm_ablation(\n",
    "    X_train_scaled, y_train_scaled, \n",
    "    X_val_scaled, y_val_scaled, \n",
    "    X_test_scaled, y_test, y_scaler,\n",
    "    lstm_param_grid\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nAblation Study Results:\")\n",
    "display(ablation_results.sort_values('RMSE'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b7ca67",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we prototyped several different models for stock price prediction:\n",
    "\n",
    "1. **Baseline Models**:\n",
    "   - Naive Last Value Model\n",
    "   - Linear Regression\n",
    "   - ARIMA\n",
    "\n",
    "2. **Advanced Models**:\n",
    "   - LSTM\n",
    "   - Transformer\n",
    "   \n",
    "We evaluated these models using several metrics (RMSE, MAE, RÂ²) and compared their performance. The LSTM and Transformer models generally outperformed the baseline models, demonstrating the value of deep learning approaches for this task.\n",
    "\n",
    "We also analyzed model complexity, training behavior, and potential overfitting issues. The ablation study framework provided insights into how different hyperparameters affect model performance.\n",
    "\n",
    "In the next steps, we could:\n",
    "1. Further tune the best-performing model\n",
    "2. Explore ensemble methods\n",
    "3. Investigate different feature engineering approaches\n",
    "4. Test on a broader range of stocks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
