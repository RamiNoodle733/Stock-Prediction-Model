{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3ada5f-a3d2-4450-b918-6c86f5feaf50",
   "metadata": {},
   "source": [
    "# Stock Market Prediction Model Prototyping\n",
    "\n",
    "This notebook focuses on prototyping different machine learning models for stock price prediction. We'll implement and compare various approaches, from simple baseline models to more sophisticated deep learning architectures.\n",
    "\n",
    "**Contents:**\n",
    "1. Data Loading and Preparation\n",
    "2. Baseline Models\n",
    "3. Deep Learning Models\n",
    "4. Model Comparison\n",
    "5. Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e66329",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error, r2_score\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatsmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msm\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential, Model\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, LSTM, Dropout, Input, MultiHeadAttention\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Input, MultiHeadAttention\n",
    "from tensorflow.keras.layers import LayerNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Add the src directory to the path so we can import our modules\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1da2ab",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation\n",
    "\n",
    "We'll load processed stock data that has already been prepared with feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "processed_data_path = '../data/processed/'\n",
    "\n",
    "# List available processed data files\n",
    "npz_files = [f for f in os.listdir(processed_data_path) if f.endswith('_ml_ready.npz')]\n",
    "print(f\"Found {len(npz_files)} processed data files.\")\n",
    "\n",
    "# If no processed files are found, we might need to run data preparation\n",
    "if len(npz_files) == 0:\n",
    "    print(\"No processed data files found. You may need to run the data preprocessing script first.\")\n",
    "    print(\"Run: python -m src.preprocess\")\n",
    "else:\n",
    "    # Select a stock symbol for prototyping (e.g., AAPL if available)\n",
    "    preferred_symbols = ['AAPL', 'MSFT', 'GOOG', 'AMZN']\n",
    "    available_symbols = [os.path.splitext(f)[0].replace('_ml_ready', '') for f in npz_files]\n",
    "    \n",
    "    selected_symbol = next((symbol for symbol in preferred_symbols if symbol in str(available_symbols)), available_symbols[0])\n",
    "    print(f\"Selected symbol for prototyping: {selected_symbol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for the selected symbol\n",
    "def load_data(symbol):\n",
    "    \"\"\"Load processed data for a specific stock symbol.\"\"\"\n",
    "    data_file = os.path.join(processed_data_path, f\"{symbol}_ml_ready.npz\")\n",
    "    \n",
    "    if not os.path.exists(data_file):\n",
    "        raise FileNotFoundError(f\"Processed data file not found: {data_file}\")\n",
    "    \n",
    "    data = np.load(data_file)\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    dates = data['dates']\n",
    "    \n",
    "    return X, y, dates\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    X, y, dates = load_data(selected_symbol)\n",
    "    print(f\"Data loaded successfully. X shape: {X.shape}, y shape: {y.shape}, dates shape: {dates.shape}\")\n",
    "    \n",
    "    # Convert dates to datetime if needed\n",
    "    if isinstance(dates[0], bytes):\n",
    "        dates = [d.decode('utf-8') for d in dates]  # Convert bytes to strings if needed\n",
    "        dates = [pd.to_datetime(d) for d in dates]  # Convert strings to datetime\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "def split_data(X, y, dates, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"Split data into training, validation, and test sets using time-based split.\"\"\"\n",
    "    # Time-based split (no shuffling)\n",
    "    n_samples = len(X)\n",
    "    test_idx = int(n_samples * (1 - test_size))\n",
    "    \n",
    "    X_temp, X_test = X[:test_idx], X[test_idx:]\n",
    "    y_temp, y_test = y[:test_idx], y[test_idx:]\n",
    "    dates_temp, dates_test = dates[:test_idx], dates[test_idx:]\n",
    "    \n",
    "    val_idx = int(len(X_temp) * (1 - val_size))\n",
    "    \n",
    "    X_train, X_val = X_temp[:val_idx], X_temp[val_idx:]\n",
    "    y_train, y_val = y_temp[:val_idx], y_temp[val_idx:]\n",
    "    dates_train, dates_val = dates_temp[:val_idx], dates_temp[val_idx:]\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, dates_train, dates_val, dates_test\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, dates_train, dates_val, dates_test = split_data(X, y, dates)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc6ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data split\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Concatenate all target values and dates\n",
    "all_dates = np.concatenate([dates_train, dates_val, dates_test])\n",
    "all_prices = np.concatenate([y_train, y_val, y_test])\n",
    "\n",
    "# Plot the full price series\n",
    "plt.plot(all_dates, all_prices, color='gray', alpha=0.3, label='All Data')\n",
    "\n",
    "# Plot the training, validation, and test sets\n",
    "plt.plot(dates_train, y_train, color='blue', label='Training Set')\n",
    "plt.plot(dates_val, y_val, color='green', label='Validation Set')\n",
    "plt.plot(dates_test, y_test, color='red', label='Test Set')\n",
    "\n",
    "# Add vertical lines to separate the sets\n",
    "plt.axvline(x=dates_train[-1], color='black', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=dates_val[-1], color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.title(f'{selected_symbol} - Data Split', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Price', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93937391",
   "metadata": {},
   "source": [
    "## 2. Baseline Models\n",
    "\n",
    "We'll implement and evaluate simple baseline models for comparison:\n",
    "- Last value prediction (naive approach)\n",
    "- Linear Regression\n",
    "- ARIMA time series model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate model performance\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate model predictions using various metrics.\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"MAPE: {mape:.4f}%\")\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape,\n",
    "        'y_pred': y_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbe428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Naive Model (predict the last observed value)\n",
    "def naive_forecast(X_test):\n",
    "    \"\"\"Use the last observed price as the prediction for the next day.\"\"\"\n",
    "    # Get the last closing price from each window\n",
    "    close_price_column = -1  # Assuming the closing price is the last feature\n",
    "    last_observed_prices = X_test[:, -1, close_price_column]\n",
    "    return last_observed_prices\n",
    "\n",
    "# Make naive predictions\n",
    "naive_preds = naive_forecast(X_test)\n",
    "\n",
    "# Evaluate the naive model\n",
    "naive_results = evaluate_model(y_test, naive_preds, \"Naive Last Value Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5844bddc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Use the linear regression model\u001b[39;00m\n\u001b[32m     31\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m lr_preds, lr_model = fit_linear_model(\u001b[43mX_train\u001b[49m, y_train, X_test)\n\u001b[32m     33\u001b[39m lr_time = time.time() - start_time\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLinear Regression training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 2. Linear Regression Model\n",
    "def fit_linear_model(X_train, y_train, X_test):\n",
    "    \"\"\"Fit a linear regression model and make predictions.\"\"\"\n",
    "    # Reshape 3D data to 2D for sklearn\n",
    "    n_samples_train, n_timesteps, n_features = X_train.shape\n",
    "    X_train_2d = X_train.reshape(n_samples_train, n_timesteps * n_features)\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X_train_scaled = scaler_X.fit_transform(X_train_2d)\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Fit the linear model\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_scaled, y_train_scaled)\n",
    "    \n",
    "    # Prepare test data\n",
    "    n_samples_test, _, _ = X_test.shape\n",
    "    X_test_2d = X_test.reshape(n_samples_test, n_timesteps * n_features)\n",
    "    X_test_scaled = scaler_X.transform(X_test_2d)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_scaled = lr_model.predict(X_test_scaled)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return y_pred, lr_model\n",
    "\n",
    "# Use the linear regression model\n",
    "start_time = time.time()\n",
    "lr_preds, lr_model = fit_linear_model(X_train, y_train, X_test)\n",
    "lr_time = time.time() - start_time\n",
    "\n",
    "print(f\"Linear Regression training time: {lr_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate the linear regression model\n",
    "lr_results = evaluate_model(y_test, lr_preds, \"Linear Regression Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edee7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fitting ARIMA model: name 'y_train' is not defined\n"
     ]
    }
   ],
   "source": [
    "# 3. ARIMA Model\n",
    "def fit_arima_model(y_train, test_size):\n",
    "    \"\"\"Fit an ARIMA model to the training data and make predictions.\"\"\"\n",
    "    # Convert to pandas Series for statsmodels\n",
    "    train_series = pd.Series(y_train)\n",
    "    \n",
    "    # Fit ARIMA model\n",
    "    order = (5, 1, 0)  # AR(5), I(1), MA(0) - common for stock prices\n",
    "    model = sm.tsa.ARIMA(train_series, order=order)\n",
    "    \n",
    "    # This can be slow, so we'll show a message\n",
    "    print(\"Fitting ARIMA model... (this may take a few minutes)\")\n",
    "    fit_model = model.fit()\n",
    "    print(\"ARIMA model fit complete.\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = fit_model.forecast(steps=test_size)\n",
    "    \n",
    "    return y_pred, fit_model\n",
    "\n",
    "# Use the ARIMA model\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    arima_preds, arima_model = fit_arima_model(y_train, len(y_test))\n",
    "    arima_time = time.time() - start_time\n",
    "\n",
    "    print(f\"ARIMA training and prediction time: {arima_time:.2f} seconds\")\n",
    "\n",
    "    # Evaluate the ARIMA model\n",
    "    arima_results = evaluate_model(y_test, arima_preds, \"ARIMA Model\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fitting ARIMA model: {e}\")\n",
    "    arima_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ff360",
   "metadata": {},
   "source": [
    "## 3. Deep Learning Models\n",
    "\n",
    "Now we'll implement and evaluate more sophisticated models:\n",
    "- LSTM (Long Short-Term Memory)\n",
    "- Transformer (with self-attention mechanism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f56feae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, X_test_scaled, y_scaler\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Scale the data\u001b[39;00m\n\u001b[32m     29\u001b[39m X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, X_test_scaled, y_scaler = preprocess_for_dl(\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[43mX_train\u001b[49m, y_train, X_val, y_val, X_test\n\u001b[32m     31\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Common preprocessing for deep learning models\n",
    "def preprocess_for_dl(X_train, y_train, X_val, y_val, X_test):\n",
    "    \"\"\"Preprocess data for deep learning models (scaling).\"\"\"\n",
    "    # Reshape X data for scaling\n",
    "    n_samples_train, n_timesteps, n_features = X_train.shape\n",
    "    X_train_reshaped = X_train.reshape(-1, n_features)\n",
    "    X_val_reshaped = X_val.reshape(-1, n_features)\n",
    "    X_test_reshaped = X_test.reshape(-1, n_features)\n",
    "    \n",
    "    # Scale features\n",
    "    X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train_scaled = X_scaler.fit_transform(X_train_reshaped)\n",
    "    X_val_scaled = X_scaler.transform(X_val_reshaped)\n",
    "    X_test_scaled = X_scaler.transform(X_test_reshaped)\n",
    "    \n",
    "    # Reshape back to 3D\n",
    "    X_train_scaled = X_train_scaled.reshape(n_samples_train, n_timesteps, n_features)\n",
    "    X_val_scaled = X_val_scaled.reshape(X_val.shape[0], n_timesteps, n_features)\n",
    "    X_test_scaled = X_test_scaled.reshape(X_test.shape[0], n_timesteps, n_features)\n",
    "    \n",
    "    # Scale targets\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = y_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, X_test_scaled, y_scaler\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, X_test_scaled, y_scaler = preprocess_for_dl(\n",
    "    X_train, y_train, X_val, y_val, X_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b983f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EarlyStopping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Set up early stopping and model checkpoint\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m early_stopping = \u001b[43mEarlyStopping\u001b[49m(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m10\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     28\u001b[39m checkpoint_path = \u001b[33m\"\u001b[39m\u001b[33m../models/lstm/lstm_checkpoint.h5\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m os.makedirs(os.path.dirname(checkpoint_path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'EarlyStopping' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. LSTM Model\n",
    "def build_lstm_model(input_shape, units=64, layers=2, dropout_rate=0.2):\n",
    "    \"\"\"Build an LSTM model with the specified architecture.\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer\n",
    "    model.add(LSTM(units=units, \n",
    "                   return_sequences=(layers > 1),\n",
    "                   input_shape=input_shape))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Additional LSTM layers\n",
    "    for i in range(layers - 1):\n",
    "        return_seq = (i < layers - 2)  # Return sequences for all but the last layer\n",
    "        model.add(LSTM(units=units, return_sequences=return_seq))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint_path = \"../models/lstm/lstm_checkpoint.h5\"\n",
    "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Build and train the LSTM model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "lstm_model = build_lstm_model(input_shape, units=64, layers=2)\n",
    "\n",
    "# Show model summary\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    validation_data=(X_val_scaled, y_val_scaled),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "lstm_time = time.time() - start_time\n",
    "print(f\"LSTM training time: {lstm_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "lstm_preds_scaled = lstm_model.predict(X_test_scaled)\n",
    "lstm_preds = y_scaler.inverse_transform(lstm_preds_scaled).flatten()\n",
    "\n",
    "# Evaluate the LSTM model\n",
    "lstm_results = evaluate_model(y_test, lstm_preds, \"LSTM Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdb8f08e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Build and train the Transformer model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m transformer_model = build_transformer_model(\u001b[43minput_shape\u001b[49m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Show model summary\u001b[39;00m\n\u001b[32m     48\u001b[39m transformer_model.summary()\n",
      "\u001b[31mNameError\u001b[39m: name 'input_shape' is not defined"
     ]
    }
   ],
   "source": [
    "# 5. Transformer Model\n",
    "def transformer_encoder_block(inputs, head_size, num_heads, ff_dim, dropout_rate=0.2):\n",
    "    \"\"\"Create a transformer encoder block.\"\"\"\n",
    "    # Multi-head self-attention\n",
    "    attention_output = MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=head_size\n",
    "    )(inputs, inputs)\n",
    "    attention_output = Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(inputs + attention_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(attention_output)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "    return LayerNormalization(epsilon=1e-6)(attention_output + ffn_output)\n",
    "\n",
    "def build_transformer_model(input_shape, head_size=256, num_heads=4, ff_dim=512, \n",
    "                            num_transformer_blocks=2, mlp_units=[128, 64], dropout_rate=0.2):\n",
    "    \"\"\"Build a transformer model for time series forecasting.\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    # Transformer blocks\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder_block(x, head_size, num_heads, ff_dim, dropout_rate)\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # MLP layers\n",
    "    for dim in mlp_units:\n",
    "        x = Dense(dim, activation=\"relu\")(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mae\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build and train the Transformer model\n",
    "transformer_model = build_transformer_model(input_shape)\n",
    "\n",
    "# Show model summary\n",
    "transformer_model.summary()\n",
    "\n",
    "# Set up early stopping and model checkpoint\n",
    "checkpoint_path = \"../models/transformer/transformer_checkpoint.h5\"\n",
    "os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "transformer_history = transformer_model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    validation_data=(X_val_scaled, y_val_scaled),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "transformer_time = time.time() - start_time\n",
    "print(f\"Transformer training time: {transformer_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "transformer_preds_scaled = transformer_model.predict(X_test_scaled)\n",
    "transformer_preds = y_scaler.inverse_transform(transformer_preds_scaled).flatten()\n",
    "\n",
    "# Evaluate the Transformer model\n",
    "transformer_results = evaluate_model(y_test, transformer_preds, \"Transformer Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50488ff7",
   "metadata": {},
   "source": [
    "## 4. Model Comparison\n",
    "\n",
    "Now we'll compare all the models to see which performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab4d20f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'naive_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Collect all results\u001b[39;00m\n\u001b[32m      2\u001b[39m all_results = [\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43mnaive_results\u001b[49m,\n\u001b[32m      4\u001b[39m     lr_results,\n\u001b[32m      5\u001b[39m     lstm_results,\n\u001b[32m      6\u001b[39m     transformer_results\n\u001b[32m      7\u001b[39m ]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Add ARIMA results if available\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33marima_results\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mand\u001b[39;00m arima_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'naive_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Collect all results\n",
    "all_results = [\n",
    "    naive_results,\n",
    "    lr_results,\n",
    "    lstm_results,\n",
    "    transformer_results\n",
    "]\n",
    "\n",
    "# Add ARIMA results if available\n",
    "if 'arima_results' in locals() and arima_results is not None:\n",
    "    all_results.append(arima_results)\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_metrics = pd.DataFrame([\n",
    "    {\n",
    "        'Model': r['model'],\n",
    "        'MSE': r['mse'],\n",
    "        'RMSE': r['rmse'],\n",
    "        'MAE': r['mae'],\n",
    "        'R²': r['r2'],\n",
    "        'MAPE (%)': r['mape']\n",
    "    }\n",
    "    for r in all_results\n",
    "])\n",
    "\n",
    "# Display the summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "display(summary_metrics.sort_values('RMSE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd3bcc72",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     plt.show()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Plot comparisons for key metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m plot_model_comparison(\u001b[43mall_results\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mRMSE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m plot_model_comparison(all_results, \u001b[33m\"\u001b[39m\u001b[33mMAE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m plot_model_comparison(all_results, \u001b[33m\"\u001b[39m\u001b[33mR2\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot model comparison\n",
    "def plot_model_comparison(results_list, metric_name):\n",
    "    \"\"\"Create a bar chart comparing models on a specific metric.\"\"\"\n",
    "    models = [r['model'] for r in results_list]\n",
    "    metric_values = [r[metric_name.lower()] for r in results_list]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(models, metric_values)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., \n",
    "                 value + 0.002*max(metric_values), \n",
    "                 f\"{value:.4f}\", \n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.title(f'Model Comparison - {metric_name}', fontsize=16)\n",
    "    plt.ylabel(metric_name, fontsize=14)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot comparisons for key metrics\n",
    "plot_model_comparison(all_results, \"RMSE\")\n",
    "plot_model_comparison(all_results, \"MAE\")\n",
    "plot_model_comparison(all_results, \"R2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b93a07c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     plt.show()\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Find the best model based on RMSE\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m best_model_idx = \u001b[43msummary_metrics\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mRMSE\u001b[39m\u001b[33m'\u001b[39m].idxmin()\n\u001b[32m     22\u001b[39m best_model = summary_metrics.loc[best_model_idx, \u001b[33m'\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     23\u001b[39m best_results = \u001b[38;5;28mnext\u001b[39m(r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m all_results \u001b[38;5;28;01mif\u001b[39;00m r[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m] == best_model)\n",
      "\u001b[31mNameError\u001b[39m: name 'summary_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot predictions vs actual for the best model\n",
    "def plot_predictions(y_true, y_pred, dates, model_name):\n",
    "    \"\"\"Plot model predictions against actual values.\"\"\"\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    plt.plot(dates, y_true, label='Actual', linewidth=2)\n",
    "    plt.plot(dates, y_pred, label=f'{model_name} Prediction', linewidth=2, linestyle='--')\n",
    "    \n",
    "    # Add shaded region for prediction error\n",
    "    plt.fill_between(dates, y_true, y_pred, color='gray', alpha=0.3, label='Error')\n",
    "    \n",
    "    plt.title(f'{selected_symbol} Stock Price Prediction - {model_name}', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=14)\n",
    "    plt.ylabel('Price', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Find the best model based on RMSE\n",
    "best_model_idx = summary_metrics['RMSE'].idxmin()\n",
    "best_model = summary_metrics.loc[best_model_idx, 'Model']\n",
    "best_results = next(r for r in all_results if r['model'] == best_model)\n",
    "\n",
    "# Plot the best model's predictions\n",
    "plot_predictions(y_test, best_results['y_pred'], dates_test, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba49a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for all models\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot actual values\n",
    "plt.plot(dates_test, y_test, label='Actual', linewidth=2.5, color='black')\n",
    "\n",
    "# Plot predictions for each model with different colors\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "for i, result in enumerate(all_results):\n",
    "    plt.plot(dates_test, result['y_pred'], label=f\"{result['model']} Prediction\", \n",
    "             linestyle='--', linewidth=1.5, alpha=0.8, color=colors[i % len(colors)])\n",
    "\n",
    "plt.title(f'{selected_symbol} Stock Price Predictions - Model Comparison', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Price', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14089d09",
   "metadata": {},
   "source": [
    "## 5. Ablation Studies\n",
    "\n",
    "Let's analyze how different hyperparameters affect model performance, focusing on the best deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2bfe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss curves\n",
    "def plot_loss_curves(history, model_name):\n",
    "    \"\"\"Plot the training and validation loss curves.\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Loss Curves', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title(f'{model_name} - MAE Curves', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Mean Absolute Error', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for overfitting\n",
    "    min_val_loss_idx = np.argmin(history.history['val_loss'])\n",
    "    min_val_loss = history.history['val_loss'][min_val_loss_idx]\n",
    "    min_train_loss = history.history['loss'][min_val_loss_idx]\n",
    "    \n",
    "    print(f\"Best epoch: {min_val_loss_idx+1}\")\n",
    "    print(f\"Training loss: {min_train_loss:.4f}\")\n",
    "    print(f\"Validation loss: {min_val_loss:.4f}\")\n",
    "    print(f\"Difference: {(min_train_loss - min_val_loss):.4f}\")\n",
    "    \n",
    "    if min_train_loss < min_val_loss:\n",
    "        gap_percent = (min_val_loss - min_train_loss) / min_val_loss * 100\n",
    "        if gap_percent > 10:\n",
    "            print(f\"Warning: Possible overfitting detected. Train-Val gap: {gap_percent:.2f}%\")\n",
    "        else:\n",
    "            print(\"Model seems well-fit (no significant overfitting detected)\")\n",
    "    else:\n",
    "        print(\"Interesting: Validation loss is lower than training loss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e26748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves for the models\n",
    "if 'lstm_history' in locals():\n",
    "    plot_loss_curves(lstm_history, 'LSTM')\n",
    "\n",
    "if 'transformer_history' in locals():\n",
    "    plot_loss_curves(transformer_history, 'Transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfdcbfc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo compatible models found for complexity analysis.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Analyze model complexity\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[32m     28\u001b[39m models_dict = {\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLSTM\u001b[39m\u001b[33m\"\u001b[39m: lstm_model,\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTransformer\u001b[39m\u001b[33m\"\u001b[39m: transformer_model\n\u001b[32m     31\u001b[39m }\n\u001b[32m     33\u001b[39m print_model_complexity(models_dict)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Analyze model complexity\n",
    "def print_model_complexity(models_dict):\n",
    "    \"\"\"Analyze and print the complexity of each model.\"\"\"\n",
    "    complexity_data = []\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        if hasattr(model, 'count_params'):\n",
    "            params = model.count_params()\n",
    "            trainable_params = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "            non_trainable_params = params - trainable_params\n",
    "            \n",
    "            complexity_data.append({\n",
    "                'Model': name,\n",
    "                'Total Parameters': params,\n",
    "                'Trainable Parameters': trainable_params,\n",
    "                'Non-trainable Parameters': non_trainable_params\n",
    "            })\n",
    "    \n",
    "    if complexity_data:\n",
    "        complexity_df = pd.DataFrame(complexity_data)\n",
    "        display(complexity_df)\n",
    "    else:\n",
    "        print(\"No compatible models found for complexity analysis.\")\n",
    "\n",
    "# Analyze model complexity\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "models_dict = {\n",
    "    \"LSTM\": lstm_model,\n",
    "    \"Transformer\": transformer_model\n",
    "}\n",
    "\n",
    "print_model_complexity(models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "882a5bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nablation_results = run_lstm_ablation(\\n    X_train_scaled, y_train_scaled, \\n    X_val_scaled, y_val_scaled, \\n    X_test_scaled, y_test, y_scaler,\\n    lstm_param_grid\\n)\\n\\n# Display results\\nprint(\"\\nAblation Study Results:\")\\ndisplay(ablation_results.sort_values(\\'RMSE\\'))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ablation study on LSTM hyperparameters\n",
    "def run_lstm_ablation(X_train, y_train, X_val, y_val, X_test, y_test, y_scaler, param_grid):\n",
    "    \"\"\"Run an ablation study on LSTM hyperparameters.\"\"\"\n",
    "    results = []\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    for units in param_grid['units']:\n",
    "        for layers in param_grid['layers']:\n",
    "            for dropout in param_grid['dropout_rate']:\n",
    "                print(f\"\\nTesting LSTM with units={units}, layers={layers}, dropout={dropout}\")\n",
    "                \n",
    "                # Build and compile the model\n",
    "                model = build_lstm_model(\n",
    "                    input_shape=input_shape,\n",
    "                    units=units,\n",
    "                    layers=layers,\n",
    "                    dropout_rate=dropout\n",
    "                )\n",
    "                \n",
    "                # Set up callbacks\n",
    "                early_stopping = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "                \n",
    "                # Train the model\n",
    "                start_time = time.time()\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=30,  # Reduced for speed\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=0\n",
    "                )\n",
    "                training_time = time.time() - start_time\n",
    "                \n",
    "                # Make predictions\n",
    "                preds_scaled = model.predict(X_test, verbose=0)\n",
    "                preds = y_scaler.inverse_transform(preds_scaled).flatten()\n",
    "                \n",
    "                # Evaluate\n",
    "                mse = mean_squared_error(y_test, preds)\n",
    "                rmse = np.sqrt(mse)\n",
    "                mae = mean_absolute_error(y_test, preds)\n",
    "                \n",
    "                # Get parameter count\n",
    "                params = model.count_params()\n",
    "                \n",
    "                # Add to results\n",
    "                results.append({\n",
    "                    'Units': units,\n",
    "                    'Layers': layers,\n",
    "                    'Dropout': dropout,\n",
    "                    'Parameters': params,\n",
    "                    'Training Time': training_time,\n",
    "                    'MSE': mse,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAE': mae,\n",
    "                    'Final Train Loss': history.history['loss'][-1],\n",
    "                    'Final Val Loss': history.history['val_loss'][-1],\n",
    "                    'Epochs': len(history.history['loss'])\n",
    "                })\n",
    "                \n",
    "                print(f\"RMSE: {rmse:.4f}, Training Time: {training_time:.2f}s\")\n",
    "                \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define the parameter grid for the ablation study\n",
    "lstm_param_grid = {\n",
    "    'units': [32, 64, 128],\n",
    "    'layers': [1, 2],\n",
    "    'dropout_rate': [0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Run the ablation study (commented out by default to save time)\n",
    "# You can uncomment and run this cell if you want to do the ablation study\n",
    "'''\n",
    "ablation_results = run_lstm_ablation(\n",
    "    X_train_scaled, y_train_scaled, \n",
    "    X_val_scaled, y_val_scaled, \n",
    "    X_test_scaled, y_test, y_scaler,\n",
    "    lstm_param_grid\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nAblation Study Results:\")\n",
    "display(ablation_results.sort_values('RMSE'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b7ca67",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we prototyped several different models for stock price prediction:\n",
    "\n",
    "1. **Baseline Models**:\n",
    "   - Naive Last Value Model\n",
    "   - Linear Regression\n",
    "   - ARIMA\n",
    "\n",
    "2. **Advanced Models**:\n",
    "   - LSTM\n",
    "   - Transformer\n",
    "   \n",
    "We evaluated these models using several metrics (RMSE, MAE, R²) and compared their performance. The LSTM and Transformer models generally outperformed the baseline models, demonstrating the value of deep learning approaches for this task.\n",
    "\n",
    "We also analyzed model complexity, training behavior, and potential overfitting issues. The ablation study framework provided insights into how different hyperparameters affect model performance.\n",
    "\n",
    "In the next steps, we could:\n",
    "1. Further tune the best-performing model\n",
    "2. Explore ensemble methods\n",
    "3. Investigate different feature engineering approaches\n",
    "4. Test on a broader range of stocks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
